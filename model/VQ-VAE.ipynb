{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e12cff5",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/dell/pycharm_remote/VQ-Font/\") # the absolute path to the code\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from six.moves import xrange\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from model import content_enc_builder\n",
    "from model import dec_builder\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image, ImageFile\n",
    "from model.modules import weights_init\n",
    "import pprint\n",
    "import json\n",
    "import tqdm"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f867e7a8",
   "metadata": {},
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "\n",
    "        self._num_embeddings = num_embeddings\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._commitment_cost = commitment_cost\n",
    "\n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.uniform_(-1 / self._num_embeddings, 1 / self._num_embeddings)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # 传入的是图片经过encoder后的feature maps\n",
    "        # convert inputs from BCHW\n",
    "        input_shape = inputs.shape\n",
    "\n",
    "        # Flatten input ->[BC HW]\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "\n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_input ** 2, dim=1, keepdim=True)\n",
    "                     + torch.sum(self._embedding.weight ** 2, dim=1)\n",
    "                     - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "\n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1) #得到编号\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "\n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "\n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized, perplexity, encodings"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc442a34",
   "metadata": {},
   "source": [
    "class VectorQuantizerEMA(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n",
    "        super(VectorQuantizerEMA, self).__init__()\n",
    "\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "\n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.normal_()\n",
    "        self._commitment_cost = commitment_cost\n",
    "\n",
    "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
    "        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n",
    "        self._ema_w.data.normal_()\n",
    "\n",
    "        self._decay = decay\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # convert inputs from BCHW -> BHWC\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "\n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "\n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_input ** 2, dim=1, keepdim=True)\n",
    "                     + torch.sum(self._embedding.weight ** 2, dim=1)\n",
    "                     - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "\n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "\n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "\n",
    "        # Use EMA to update the embedding vectors\n",
    "        if self.training:\n",
    "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
    "                                     (1 - self._decay) * torch.sum(encodings, 0)\n",
    "\n",
    "            # Laplace smoothing of the cluster size\n",
    "            n = torch.sum(self._ema_cluster_size.data)\n",
    "            self._ema_cluster_size = (\n",
    "                    (self._ema_cluster_size + self._epsilon)\n",
    "                    / (n + self._num_embeddings * self._epsilon) * n)\n",
    "\n",
    "            dw = torch.matmul(encodings.t(), flat_input)\n",
    "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
    "\n",
    "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
    "\n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        loss = self._commitment_cost * e_latent_loss\n",
    "\n",
    "        # Straight Through Estimator\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdd437fb",
   "metadata": {},
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay=0):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self._encoder = content_enc_builder(1,32,256)\n",
    "\n",
    "        if decay > 0.0:\n",
    "            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim,\n",
    "                                              commitment_cost, decay)\n",
    "        else:\n",
    "            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n",
    "                                           commitment_cost)\n",
    "        self._decoder = dec_builder(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self._encoder(x) #[B 256 16 16]\n",
    "        loss, quantized, perplexity, _ = self._vq_vae(z)\n",
    "        x_recon = self._decoder(quantized)\n",
    "\n",
    "        return loss, x_recon, perplexity\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59573f1c",
   "metadata": {},
   "source": [
    "class CombTrain_VQ_VAE_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    CombTrain_VQ_VAE_dataset, learn the laten codebook from content font. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, transform = None):\n",
    "        self.img_path = root\n",
    "        self.transform = transform\n",
    "        self.imgs = self.read_file(self.img_path)\n",
    "        # img = Image.open(self.imgs[0])\n",
    "        # img = self.transform(img)\n",
    "        # print(img.shape)\n",
    "\n",
    "\n",
    "    def read_file(self, path):\n",
    "        \"\"\"从文件夹中读取数据\"\"\"\n",
    "        files_list = os.listdir(path)\n",
    "        file_path_list = [os.path.join(path, img) for img in files_list]\n",
    "        file_path_list.sort()\n",
    "        return file_path_list\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.imgs[index]\n",
    "        #print(img_name[-5:-4])\n",
    "        img = Image.open(img_name)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img) #Tensor [C H W] [1 128 128]\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.imgs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b99062",
   "metadata": {},
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_imgs_path = 'path/to/save/train_content_imgs/'\n",
    "tensorize_transform = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                          transforms.ToTensor()])\n",
    "\n",
    "train_dataset = CombTrain_VQ_VAE_dataset(train_imgs_path, transform=tensorize_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, batch_sampler=None, drop_last=True, pin_memory=True, shuffle=True)\n",
    "\n",
    "num_training_updates = 50000\n",
    "\n",
    "embedding_dim = 256\n",
    "num_embeddings = 100\n",
    "\n",
    "commitment_cost = 0.25\n",
    "\n",
    "decay = 0\n",
    "\n",
    "learning_rate = 2e-4\n",
    "\n",
    "model = Model(num_embeddings, embedding_dim, commitment_cost, decay).to(device)\n",
    "model.apply(weights_init(\"xavier\"))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=False)\n",
    "\n",
    "model.train()\n",
    "train_res_recon_error = []\n",
    "train_res_perplexity = []\n",
    "train_vq_loss = []\n",
    "\n",
    "\n",
    "def val(model,validation_loader):\n",
    "    model.eval()\n",
    "\n",
    "    valid_originals = next(iter(validation_loader))\n",
    "    valid_originals = valid_originals.to(device)\n",
    "\n",
    "    vq_output_eval = model._encoder(valid_originals)\n",
    "    _, valid_quantize, _, _ = model._vq_vae(vq_output_eval)\n",
    "    valid_reconstructions = model._decoder(valid_quantize)\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    fig = plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "for i in xrange(num_training_updates):\n",
    "    data = next(iter(train_loader))\n",
    "    train_data_variance = torch.var(data)\n",
    "    # print(train_data_variance)\n",
    "    # show(make_grid(data.cpu().data) )\n",
    "    # break\n",
    "    data = data - 0.5 # normalize to [-0.5, 0.5]\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    vq_loss, data_recon, perplexity = model(data)\n",
    "    # data_recon重构图像\n",
    "    # print(\"vq_loss\\n\",vq_loss)\n",
    "    recon_error = F.mse_loss(data_recon, data) / train_data_variance\n",
    "    loss = recon_error + vq_loss\n",
    "    # 重构损失更新encoder以及decoder,vq_loss用来更新embedding空间\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_res_recon_error.append(recon_error.item())\n",
    "    train_res_perplexity.append(perplexity.item())\n",
    "    train_vq_loss.append(vq_loss.item())\n",
    "\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print('%d iterations' % (i + 1))\n",
    "        print('recon_error: %.3f' % np.mean(train_res_recon_error[-1000:]))\n",
    "        print('perplexity: %.3f' % np.mean(train_res_perplexity[-1000:]))\n",
    "        print('vq_loss: %.3f' % np.mean(train_vq_loss[-1000:]))\n",
    "        print()\n",
    "        # show(make_grid(data.cpu().data) )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf4ba5cd",
   "metadata": {},
   "source": [
    "val_imgs_path = 'path/to/save/val_content_imgs'\n",
    "tensorize_transform = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                          transforms.ToTensor()])\n",
    "\n",
    "val_dataset = CombTrain_VQ_VAE_dataset(val_imgs_path, transform=tensorize_transform)\n",
    "\n",
    "validation_loader = DataLoader(val_dataset, batch_size=8, batch_sampler=None, drop_last=True, pin_memory=True, shuffle=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8453a418",
   "metadata": {},
   "source": [
    "def val_(model,validation_loader):\n",
    "    model.eval()\n",
    "\n",
    "    valid_originals = next(iter(validation_loader))\n",
    "    valid_originals = valid_originals.to(device)\n",
    "\n",
    "    vq_output_eval = model._encoder(valid_originals)\n",
    "    _, valid_quantize, _, _ = model._vq_vae(vq_output_eval)\n",
    "    valid_reconstructions = model._decoder(valid_quantize)\n",
    "    return valid_originals, valid_reconstructions\n",
    "    \n",
    "org, recon_out = val_(model, validation_loader)\n",
    "show(make_grid((org+0.5).cpu().data), )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "239b2606",
   "metadata": {},
   "source": [
    "show(make_grid((recon_out+0.5).cpu().data), )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a5e55be",
   "metadata": {},
   "source": [
    "torch.save(model,'../weight/VQ-VAE_chn_.pth')    #保存所有的网络参数\n",
    "torch.save(model.state_dict(),'../weight/VQ-VAE_Parms_chn_.pth')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec4963c",
   "metadata": {},
   "source": [
    "embedding_dim, num_embeddings, commitment_cost, decay = 256, 100, 0.25, 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model(num_embeddings, embedding_dim, commitment_cost, decay).to(device)\n",
    "models = torch.load('../weight/VQ-VAE_chn_.pth')\n",
    "encoder = models._encoder\n",
    "encoder.requires_gradq = False\n",
    "encoder.to(\"cpu\")\n",
    "#定义dataset\n",
    "class CombTrain_VQ_VAE_dataset(Dataset):\n",
    "    def __init__(self, root, transform = None):\n",
    "        self.img_path = root\n",
    "        self.transform = transform\n",
    "        self.imgs = self.read_file(self.img_path)\n",
    "        # img = Image.open(self.imgs[0])\n",
    "        # img = self.transform(img)\n",
    "        # print(img.shape)\n",
    "\n",
    "\n",
    "    def read_file(self, path):\n",
    "        \"\"\"从文件夹中读取数据\"\"\"\n",
    "        files_list = os.listdir(path)\n",
    "        file_path_list = [os.path.join(path, img) for img in files_list]\n",
    "        file_path_list.sort()\n",
    "        return file_path_list\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.imgs[index]\n",
    "        img = Image.open(img_name)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img) #Tensor [C H W] [1 128 128]\n",
    "        ret =(img_name, \n",
    "              img\n",
    "        )\n",
    "        return ret\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_imgs_path = 'path/to/save/all_content_imgs'\n",
    "tensorize_transform = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                          transforms.ToTensor()])\n",
    "\n",
    "batch = 3500 # all content imgs\n",
    "\n",
    "sim_dataset = CombTrain_VQ_VAE_dataset(train_imgs_path, transform=tensorize_transform)\n",
    "\n",
    "sim_loader = DataLoader(sim_dataset, batch_size=batch, batch_sampler=None, drop_last=False, pin_memory=True)\n",
    "\n",
    "similarity = []\n",
    "\n",
    "def CosineSimilarity(tensor_1, tensor_2):\n",
    "    normalized_tensor_1 = tensor_1 / tensor_1.norm(dim=-1, keepdim=True)\n",
    "    normalized_tensor_2 = tensor_2 / tensor_2.norm(dim=-1, keepdim=True)\n",
    "    return (normalized_tensor_1 * normalized_tensor_2).sum(dim=-1)\n",
    "\n",
    "\n",
    "while True:\n",
    "    data = next(iter(sim_loader))\n",
    "    img_name = data[0]\n",
    "    img_tensor = data[1]\n",
    "    img_tensor = img_tensor - 0.5 # normalize to [-0.5, 0.5]\n",
    "    img_tensor = img_tensor.to(\"cpu\")\n",
    "    \n",
    "    #得到了conten的feature\n",
    "    content_feature = encoder(img_tensor)\n",
    "    # print(content_feature.shape)\n",
    "    vector = content_feature.view(content_feature.shape[0], -1)\n",
    "    # print(vector.shape)\n",
    "    \n",
    "    sim_all = {}\n",
    "    for i in range(0,batch):\n",
    "        char_i = hex(ord(img_name[i][-5]))[2:].upper()\n",
    "        dict_sim_i = {char_i:{}}\n",
    "        for j in range(0,batch):\n",
    "            char_j = hex(ord(img_name[j][-5]))[2:].upper()\n",
    "            similarity = CosineSimilarity(vector[i],vector[j])\n",
    "            if i==j:\n",
    "                similarity=1.0\n",
    "            sim_i2j = {char_j:float(similarity)}\n",
    "            dict_sim_i[char_i].update(sim_i2j)\n",
    "        sim_all.update(dict_sim_i)\n",
    "\n",
    "        \n",
    "    dict_json=json.dumps(sim_all)#转化为json格式文件\n",
    "\n",
    "    #将json文件保存为.json格式文件\n",
    "    with open('../weight/all_char_similarity_unicode.json','w+') as file:\n",
    "        file.write(dict_json)    \n",
    "    break"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9318891b",
   "metadata": {},
   "source": [
    "with open('../weight/all_char_similarity_unicode.json','r+') as file:\n",
    "    content=file.read()\n",
    "    \n",
    "content=json.loads(content)#将json格式文件转化为python的字典文件\n",
    "print(content['4E08'])"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fontgen",
   "language": "python",
   "name": "fontgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
